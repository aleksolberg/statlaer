---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Compulsory exercise 1: Group 5"
author: "Aleksander Johnsen Solberg and Gjermund Oscar Lyckander"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")
```

```{r,eval=TRUE,echo=FALSE}
#install.packages("Rtools")
#install.packages("knitr") #probably already installed
#install.packages("rmarkdown") #probably already installed
#install.packages("ggplot2") #plotting with ggplot
#install.packages("palmerpenguins")
#install.packages("ggfortify") # For model checking
#install.packages("MASS")
#install.packages("class")
#install.packages("pROC")
#install.packages("plotROC")
#install.packages("boot")
#install.packages('plyr', repos = "http://cran.us.r-project.org")
library("knitr")
#install.packages("tinytex")
#tinytex::install_tinytex()
library("rmarkdown")
library("palmerpenguins")
```

<!--  Etc (load all packages needed). -->
  
  
# Problem 1
  
## a)
  $$
  \begin{aligned}
E[y_0 - \hat f(x_0)]^2 &= E[(f(x_0) + \varepsilon - \hat f(x_0))^2] \\ 
&= E[(f(x_0))^2] + E[\varepsilon^2] + E[\hat f(x_0)^2] -2E[f(x_0) \hat f(x_0)] + 2E[f(x_0) \varepsilon] + 2E[ \hat f(x_0) \varepsilon] \\
&= f(x_0)^2 + \text{Var}(\varepsilon) + \text{Var}(\hat f(x_0)) + E[\hat f(x_0)]^2 - 2E[f(x_0) \hat f(x_0)]\\
&= (f(x_0) - E[\hat f(x_0)])^2 + \text{Var}(\hat f(x_0))  + \text{Var}(\varepsilon)
\end{aligned}
$$
  The three terms in the last line are the squared bias, variance, and irreducible error respectively.

## b)
The three terms can be interpreted as the following.
The bias term is the error that comes from modeling a complicated real-life problem with a simple model. The more flexible the model is, the smaller the bias will be. 
The variance term is how much the estimate $\hat{f}$ would change if we were using different training data. The more flexible the model is, the larger the variance will be.
Lastly, the irreducible error term is simply the error that comes from the error in the data itself.

## c)
(i) True
(ii) False
(iii) True
(iv) False

## d)
(i) True
(ii) False
(iii) False
(iv) False

## e)
(i) is true


# Problem 2

Here is a code chunk:
  
```{r, eval=TRUE}
library(palmerpenguins) # Contains the data set "penguins".
data(penguins)
head(penguins)
Penguins <- subset(penguins, select = -c(island, year))
```


## a)
* Takes the covariate 'sex' out of the model despite it being very segnificant. Basil clearly has clearly misunderstood what is considered a good p-value
* Leaves in covariates that are clearly not significant, such as the interaction between 'bill_depth_mm' and 'species'.
* Says that the interaction term is overall significant when only this is only true for one species.
* Does not include 'bill_length_mm' in the model at any point, even though we suspect it might be siginificant
* Concludes that chinstrap penguins have the largest bodymass, which one can clearly see from the data is not true. There must be something wrong with the model.

## b) 
```{r, eval=TRUE}
library(GGally)
ggpairs(Penguins, aes(colour = species))
```


## c)
```{r, eval=TRUE}
penguin.model1 <- lm(body_mass_g ~ . + species*bill_depth_mm, data = Penguins)
summary(penguin.model1)
anova(penguin.model1)
```
Not quite happy with this, as the species coavriate is not as significant as one would think from seeing the pairs plot. Try without 'bill_length_mm' and the interactions.
```{r, eval=TRUE}
penguin.model2 <- lm(body_mass_g ~  bill_depth_mm + flipper_length_mm + sex + species, data = Penguins)
summary(penguin.model2)
anova(penguin.model2)
```
We begin with a model with bodey mass as the response, and species, bill length, bill depth, flipper length, sex and the interaction between species and bill depth as the covariates. We see that not all of these predictors are significant in the model, and so we try a reduced model using only bill length, flipper length, sex and species as the covariates. We now see that all the covariates have p-values that are very low, and so they should be included in the model. In the species covariate, we see that only the distinction between Gentoo and the other two species is helpful, and so we will reflect this in the model by only distinguishing between Gentoo and not Gentoo. The final model can thus be described as such:
  $$
  \begin{aligned}
\hat{y}_{female} &= \hat{\beta}_0 + \hat{\beta}_{bill\_depth} x_{bill\_depth} + \hat{\beta}_{flipper\_length} x_{flipper\_length}\\
\hat{y}_{male} &= \hat{\beta}_0 + \hat{\beta}_{bill\_depth} x_{bill\_depth} + \hat{\beta}_{flipper\_length} x_{flipper\_length} + \hat{\beta}_{male}\\
\hat{y}_{female\_gentoo} &= \hat{\beta}_0 + \hat{\beta}_{bill\_depth} x_{bill\_depth} + \hat{\beta}_{flipper\_length} x_{flipper\_length} + \hat{\beta}_{gentoo}\\
\hat{y}_{male\_gentoo} &= \hat{\beta}_0 + \hat{\beta}_{bill\_depth} x_{bill\_depth} + \hat{\beta}_{flipper\_length} x_{flipper\_length} + \hat{\beta}_{male} + \hat{\beta}_{gentoo}
\end{aligned}
$$
```{r}
library(ggfortify)
autoplot(penguin.model2)
```
From the residuals vs. fitted plot, we do not see any evidence of non-linearity. and can therefore conclude that the expected value of the residuals is zero. We do however see some structure in that the points are grouped together into four groups. This might come from the fact that we have a model that is split into four given by the sex and the species, however, the significance of this structure is unknown. 

In the QQ-plot, we can see the points follow the straight line very well, and we can say that the residuals are normally distributed.

# Problem 3
```{r}
library(tidyverse)
library(GGally)
# Create a new boolean variable indicating whether or not the penguin is an Adelie penguin
Penguins$adelie <- ifelse(Penguins$species == "Adelie", 1, 0)
# Select only relevant variables and remove all rows with missing values in 
# body mass, flipper length, sex or species.
Penguins_reduced <- Penguins %>% 
  dplyr::select(body_mass_g, flipper_length_mm, adelie) %>% 
  mutate(body_mass_g = as.numeric(body_mass_g), 
         flipper_length_mm = as.numeric(flipper_length_mm)) %>% 
  drop_na()
set.seed(4268) 
# 70% of the sample size for training set
training_set_size <- floor(0.70 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```
## a)
  (i)
```{r}
ggpairs(Penguins_reduced)
log.model <- glm(adelie ~ ., data = train, family = binomial)
summary(log.model)
```
```{r}
log.probabilities <- predict(log.model, newdata = test, type = 'response')
log.predicted.classes <- ifelse(log.probabilities > 0.5, 1, 0)
mean(log.predicted.classes == test$adelie)
```
  (ii)
```{r}
library(MASS)
qda.model <- qda(adelie ~ ., data = train)
summary(log.model)
```
```{r}
qda.probabilities <- predict(qda.model, newdata = test, type = 'response')$posterior
qda.predicted.classes <- predict(qda.model, newdata = test, type = 'response')$class
mean(qda.predicted.classes == test$adelie)
```
  (iii)
```{r}
library(class)
knn.model <- knn(train = train, test = test, cl = train$adelie, k=25, prob = T)
mean(knn.model == test$adelie)
```
  (iv)
```{r}
library(caret)
sensitivity(table(log.predicted.classes, test$adelie))
specificity(table(log.predicted.classes, test$adelie))
```
```{r}
sensitivity(table(qda.predicted.classes, test$adelie))
specificity(table(qda.predicted.classes, test$adelie))
```
```{r}
sensitivity(table(knn.model, test$adelie))
specificity(table(knn.model, test$adelie))
```
The logistic regression model has a sensitivity of $87\%$, and a specificity of $98\%$.
The QDA model has a sensitivity of $77\%$, and a specificity of $98\%$.
The KNN model has a sensitivity of $58\%$, and a specificity of $95\%$.

## b)
(i)
```{r}
library(pROC)
log.roc <- roc(test$adelie, log.probabilities, direction = '<', lwd=3)
plot(log.roc)
auc(log.roc)
```
```{r}
qda.roc = roc(test$adelie, qda.probabilities[,2], direction = '<', lwd=3)
plot(qda.roc)
auc(qda.roc)
```
```{r}
probKNN = ifelse(knn.model == 0, 1 - attributes(knn.model)$prob, attributes(knn.model)$prob)
knn.roc <- roc(test$adelie, probKNN, direction = '<', lwd=3)
plot(knn.roc)
auc(knn.roc)
```
(ii)
We see that the logistic regression model and the QDA model both perform very well in this instance, while the KNN model does a decent job, but far worse than the other two.

(iii)
In order to get an interpretable model, I would choose the logistic regression model, as you can easily interpret the effect each covariate has on the prediction from the coefficients the model gives.

## c)
(iii) is True

## d)
```{r}
library(ggplot2)
log.train.probabilities <- predict(log.model, newdata = train, type = 'response')
log.train.predicted.classes <- ifelse(log.train.probabilities > 0.5, 1, 0)
ggplot(test, aes(x = body_mass_g, y = flipper_length_mm, group=adelie)) +
      geom_point(aes(colour = factor(adelie, labels = c('Not Adelie', 'Adelie')), shape = factor(log.predicted.classes, labels=c('Not Adelie', 'Adelie')))) + 
      geom_point(data=train, aes(colour = factor(adelie, labels = c('Not Adelie', 'Adelie')), shape = factor(log.train.predicted.classes, labels=c('Not Adelie', 'Adelie')))) +
      labs(color = 'True species', shape = 'Predicted') +
      theme_bw()
  
```


# Problem 4

## a)
(i) True
(ii) False
(iii) False
(iv) False

## b)

```{r, eval=TRUE}
id <- "1chRpybM5cJn4Eow3-_xwDKPKyddL9M2N" # google file ID
d.chd <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))
d.chd$sex <- as.factor(d.chd$sex)
d.chd$smoking <- as.factor(d.chd$smoking)
glm.fit <- glm(chd ~ sbp + sex + smoking, data = d.chd, family = "binomial")
summary(glm.fit)$coef
glm.predict <- predict(glm.fit, data.frame(sbp = 150, sex = as.factor(1), smoking = as.factor(0)), type = "response")
glm.predict
```

The probability of chd for a non-smoking male with a sbp=150 is 0.101.

## c)

```{r, eval=TRUE}
B <- 1000
n <- 101
estimate <- rep(NA, B)
for (b in 1:B){
  set.seed(b)
  thisboot <- d.chd[sample(nrow(d.chd), n), ]
  boot.fit <- glm(chd ~ sbp + sex + smoking, data = thisboot, family = "binomial")
  boot.predict <- predict(boot.fit, data.frame(sbp = 150, sex = as.factor(1), smoking = as.factor(0)), type = "response")
  estimate [b] <- boot.predict
}

std.err <- function(x) sd(x)/sqrt(length(x))
estimate.stderr <- std.err(estimate)

estimate.mean <- mean(estimate)

alpha <- 0.05
degrees.freedom <- length(estimate)-3
t.score = qt(p=alpha/2, df = degrees.freedom, lower.tail = F)

margin.error <- t.score*estimate.stderr
lower.bound <- estimate.mean - margin.error
upper.bound <- estimate.mean + margin.error


estimate.stderr

estimate.mean

lower.bound

upper.bound


```

The standard error is `r estimate.stderr`.

The 95% quantile interval is (`r lower.bound`, `r upper.bound`).

From the results of the bootstrap we see that a non-smoking male with a sbp=150 has an expected probability of coronary heart disease of `r estimate.mean`. There is a probability of 95% that the expected probability of chd for a non-smoking male with a sbp=150 lies between `r lower.bound` and `r upper.bound`.


## d)
(i) False
(ii) True
(iii) False
(iv) False
